// index.js
import express from "express";
import cors from "cors";
import bodyParser from "body-parser";
import { spawn } from "child_process";
import fetch from "node-fetch";
import { Readable } from "stream";

const app = express();
const port = process.env.PORT || 3000;

// Ollama (local LLaMA via Ollama API)
const OLLAMA_API_URL =
  process.env.OLLAMA_API_URL || "http://ollama:11434/v1/chat/completions";
const LLAMA_MODEL = process.env.LLAMA_MODEL || "llama3";

// OpenAI configuration
const OPENAI_API_URL = "https://api.openai.com/v1/chat/completions";
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const OPENAI_MODEL = process.env.OPENAI_MODEL || "gpt-4.1-nano";

app.use(bodyParser.json());
app.use(cors());

// 1) Minimaler System-Prompt: Rolle & Zweck
const SYSTEM_PROMPT = `
You are **ChatUML**, a teaching assistant that converts plain-language
requirements into clear, self-explanatory PlantUML diagrams (including UML, mindmaps, and more).
Always follow the user‚Äôs instructions exactly.
`.trim();

// 2) Prompt-Template mit Platzhaltern f√ºr Regeln, Beispiele und Anwender-Anfrage
const PROMPT_TEMPLATE = `
=== Conversion Task ===
Convert the following requirement into a PlantUML diagram or mindmap:

{user_input}

=== Rules ===
1. **Single block**: Output exactly one PlantUML block.  
   ‚Ä¢ Begin with \`@startuml\`/\`@startmindmap\`, end with \`@enduml\`/\`@endmindmap\`.  
   ‚Ä¢ No additional text or Markdown outside this block.  
2. **Diagram type**: Choose the most suitable type automatically:  
   ‚Äì Structural: class, component, deployment, object, ER  
   ‚Äì Behavioural: sequence, activity, state, use-case, timing  
   ‚Äì Other: mindmap  
3. **Clarity & Teaching**:  
   ‚Äì Use meaningful names, stereotypes ¬´ ¬ª, labels, notes.  
   ‚Äì Show relationships (multiplicity, directions, messages, transitions) or hierarchy for mindmaps.  
   ‚Äì Add a concise title or legend if helpful.  
4. **Syntax cheatsheet** (for reference only):  
   ‚Äì Class: \`class Order { +id : UUID \\\\n +submit() }\`  
   ‚Äì Sequence:  
   \`\`\`  
   actor User  
   User -> Auth : login()  
   Auth --> User : JWT  
   \`\`\`  
   ‚Äì Mindmap:  
   \`\`\`  
   @startmindmap  
   * RootTopic  
   ** SubTopic1  
   ** SubTopic2  
   @endmindmap  
   \`\`\`  
5. **No styling**: Avoid skinparams, themes or headers unless explicitly requested.

=== Examples ===
{examples}

=== Your Diagram ===
`.trim();

// 3) Beispiel-Platzhalter (f√ºlle ‚Äû{examples}‚Äú sp√§ter mit konkreten Beispielen)
const EXAMPLES = `
// 1) Class Diagram ‚Äì Library System
@startuml
class Student {
  +Name: String
}
class Course {
  +Title: String
}
Student "0..*" - "1..*" Course
class Enrollment {
}
(Student, Course) .. Enrollment
@enduml

// 2) Sequence Diagram ‚Äì Login Flow
@startuml
actor User
participant Auth
database DB
User -> Auth : submitCredentials(email, pwd)
Auth -> DB : SELECT * FROM users
DB --> Auth : userRecord
Auth -> User : JWT
@enduml

// 3) Activity Diagram ‚Äì Order Process
@startuml
start
:Place Order;
if (In Stock?) then (yes)
  :Ship Item;
else (no)
  :Notify Customer;
endif
stop
@enduml

// 4) Use-Case Diagram ‚Äì Library System
@startuml
actor "Member" as Member
(usecase "Borrow Book") as UC1
(usecase "Return Book") as UC2
Member -- UC1
Member -- UC2
@enduml

// 5) State Diagram ‚Äì Traffic Light
@startuml
[*] --> Red
Red --> Green : timer / switch
Green --> Yellow : timer / switch
Yellow --> Red : timer / switch
@enduml

// 6) Component Diagram ‚Äì E-Commerce
@startuml
component "Web Frontend" as Web
component "API Service" as API
component "Database" as DB
Web --> API : REST
API --> DB : SQL
@enduml

// 7) Deployment Diagram ‚Äì Cloud Architecture
@startuml
node "Load Balancer" as LB {
}
node "Web Server" as WS {
  component Web
}
node "Database Server" as DS {
  database DB
}
LB --> WS
WS --> DS
@enduml

// 8) ER Diagram ‚Äì Information Model
@startuml
entity Customer {
  *customer_id : int
  --
  name : string
}
entity Order {
  *order_id : int
  --
  date : date
}
Customer ||--o{ Order : places
@enduml

// 9) Object Diagram ‚Äì Instance Snapshot
@startuml
object person1 {
  name = "Alice"
  age = 30
}
object person2 {
  name = "Bob"
  age = 25
}
person1 -- person2 : knows
@enduml

// 10) Timing Diagram ‚Äì Signal States
@startuml
robust "WebBrowser" as WB
concise "WebUser" as WU
@0
WU is Idle
WB is Idle
@+100
WU -> WB : request()
WB is Processing
@+200
WB is Idle
@enduml

// 11) Mindmap ‚Äì K√ºnstliche Intelligenz
@startmindmap
title K√ºnstliche Intelligenz ‚Äì Komponenten und Bereiche
* K√ºnstliche Intelligenz
** Grundlagen
*** Definition
*** Geschichte
*** Zielsetzung
** Teilgebiete
*** Maschinelles Lernen
*** Deep Learning
*** Nat√ºrliche Sprachverarbeitung
*** Computer Vision
*** Robotik
*** Expertensysteme
** Anwendungen
*** Sprachassistenten
*** Bild- und Spracherkennung
*** Autonomes Fahren
*** Medizinische Diagnostik
*** Empfehlungssysteme
** Technologien
*** Neuronale Netze
*** Entscheidungsb√§ume
*** Support Vector Machines
*** Reinforcement Learning
*** Big Data
*** Cloud Computing
@endmindmap
`.trim();

// 4) Helper zum Bef√ºllen des Templates
function buildPrompt(userInput) {
  return PROMPT_TEMPLATE.replace("{user_input}", userInput).replace(
    "{examples}",
    EXAMPLES
  );
}
// Helper: call local Ollama (LLaMA)
async function callLlama(prompt, temperature = 0.7) {
  console.log("Calling LLaMA with prompt:", prompt);
  const fullPrompt = buildPrompt(prompt);
  const resp = await fetch(OLLAMA_API_URL, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model: LLAMA_MODEL,
      messages: [
        { role: "system", content: SYSTEM_PROMPT },
        { role: "user", content: fullPrompt },
      ],
    }),
  });
  if (!resp.ok) throw new Error(`Ollama API error ${resp.status}`);
  const { choices } = await resp.json();
  return choices[0].message.content.trim();
}

// Helper: call OpenAI Chat
async function callOpenAI(prompt, temperature = 0.7) {
  if (!OPENAI_API_KEY) throw new Error("Missing OPENAI_API_KEY");
  console.log("Calling OpenAI with prompt:", prompt);
  const fullPrompt = buildPrompt(prompt);
  const resp = await fetch(OPENAI_API_URL, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${OPENAI_API_KEY}`,
    },
    body: JSON.stringify({
      model: OPENAI_MODEL,
      messages: [
        { role: "system", content: SYSTEM_PROMPT },
        { role: "user", content: fullPrompt },
      ],
      temperature,
    }),
  });
  if (!resp.ok) throw new Error(`OpenAI API error ${resp.status}`);
  const { choices } = await resp.json();
  return choices[0].message.content.trim();
}

// PlantUML rendering: via Java JAR
async function renderPlantUML(umlText, format = "svg") {
  return new Promise((resolve, reject) => {
    const args = [
      "-jar",
      "/usr/local/bin/plantuml.jar",
      "-pipe",
      format === "svg" ? "-tsvg" : "-tpng",
    ];
    const proc = spawn("java", args, { stdio: ["pipe", "pipe", "inherit"] });
    const chunks = [];
    proc.stdout.on("data", (b) => chunks.push(b));
    proc.on("close", (code) => {
      if (code === 0) resolve(Buffer.concat(chunks));
      else reject(new Error(`PlantUML exited with code ${code}`));
    });
    proc.stdin.write(umlText);
    proc.stdin.end();
  });
}

/**
 * POST /api/prompt
 * Request JSON:
 *   { prompt: string,             // User prompt for diagram generation (required)
 *     model?: 'llama'|'openai',   // Choose backend (default: 'llama')
 *     temperature?: number        // Sampling temperature 0.0 - 1.0 (default: 0.7)
 *   }
 * Response JSON:
 *   { success: true, plantuml: string }
 *   or on error { success: false, error: string }
 */

app.post("/api/prompt", async (req, res) => {
  console.log("Received prompt:", req.body);
  const { prompt, model = "llama", temperature = 0.7 } = req.body;
  if (!prompt || typeof prompt !== "string" || !prompt.trim()) {
    return res
      .status(400)
      .json({ error: "Prompt must be a non-empty string." });
  }

  try {
    // 1) kompletten PlantUML-String holen
    const plantuml =
      model === "gpt4o"
        ? await callOpenAI(prompt, temperature)
        : await callLlama(prompt, temperature);

    res.setHeader("Content-Type", "text/plain; charset=utf-8");
    res.setHeader("Transfer-Encoding", "chunked");
    res.flushHeaders();

    const CHUNK_SIZE = 32; // gr√∂√üer, damit nicht zu viele Chunks
    for (let i = 0; i < plantuml.length; i += CHUNK_SIZE) {
      const chunk = plantuml.slice(i, i + CHUNK_SIZE);
      res.write(chunk);
      // kurze Pause, damit auch Proxy/Browser nicht alles puffert:
      await new Promise((r) => setTimeout(r, 50));
    }
    res.end();
  } catch (err) {
    console.error("LLM error", err);
    // auf Error-Fall als JSON antworten
    res.status(500).json({ error: err.message });
  }
});

/**
 * POST /api/uml
 * Request JSON:
 *   { plantuml: string,           // PlantUML source generated from /api/prompt (required)
 *     download?: boolean,         // If true, triggers file download (default: false)
 *     format?: 'png'|'svg'        // Output format for download mode (default: 'png')
 *   }
 * Response:
 *   - Inline SVG (download=false):
 *       Content-Type: image/svg+xml
 *       <svg>...</svg>
 *   - Download mode (download=true):
 *       Content-Type: image/png|image/svg+xml
 *       Content-Disposition: attachment; filename="diagram.png|svg"
 *       Binary PNG or SVG text
 *   - On error: JSON { success: false, error: string, plantuml?: string }
 */
app.post("/api/uml", async (req, res) => {
  const { plantuml, download = false, format = "png" } = req.body;

  console.log("üì• Neue /api/uml Anfrage erhalten");
  console.log("üîé Anfrage-Parameter:", { download, format });

  if (!plantuml || typeof plantuml !== "string") {
    console.warn("‚ö†Ô∏è Ung√ºltiger oder fehlender plantuml-String");
    return res.status(400).json({ error: "plantuml must be a string." });
  }
  if (download && !["png", "svg"].includes(format)) {
    console.warn("‚ö†Ô∏è Ung√ºltiges Format im Download-Modus:", format);
    return res.status(400).json({ error: "format must be png or svg." });
  }
  try {
    if (!download) {
      // Inline embed always SVG
      console.log("üìÑ Inline-Modus aktiv ‚Äì rendere SVG");
      const buf = await renderPlantUML(plantuml, "svg");
      res.setHeader("Content-Type", "image/svg+xml");
      console.log("‚úÖ SVG gerendert, sende Antwort");
      return res.send(buf.toString("utf8"));
    }
    // Download mode
    console.log(`üíæ Download-Modus aktiv ‚Äì rendere ${format.toUpperCase()}`);
    const buf = await renderPlantUML(plantuml, format);
    const mime = format === "svg" ? "image/svg+xml" : "image/png";
    const ext = format;
    res.setHeader("Content-Type", mime);
    res.setHeader(
      "Content-Disposition",
      `attachment; filename="diagram.${ext}"`
    );

    console.log(`üíæ Download-Modus aktiv ‚Äì rendere ${format.toUpperCase()}`);
    if (format === "svg") res.send(buf.toString("utf8"));
    else res.send(buf);
  } catch (err) {
    console.error("Render error", err);
    res.status(500).json({ success: false, error: err.message, plantuml });
  }
});

app.listen(port, () => {
  console.log(`Backend listening at http://localhost:${port}`);
});
