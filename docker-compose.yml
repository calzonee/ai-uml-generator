version: "3.9"
services:
  localai:
    image: localai/localai:latest-cpu
    container_name: localai
    ports:
      - "8080:8080"
    environment:
      - MODELS_PATH=/models
      - MODELS=phi-2
      - THREADS=8
      - DISABLE_MLOCK=true
      - DEBUG=true
      - LOCALAI_ENV_DISABLE=1

    volumes:
      - ./models:/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 30s
      retries: 5
    restart: unless-stopped

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: chatuml-backend
    image: project-backend:latest
    ports:
      - "3001:3000"
    environment:
      - LLM_API_URL=http://localai:8080/v1/chat/completions
      - LLM_MODEL=llama3-8b
    depends_on:
      - localai
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: chatuml-frontend
    ports:
      - "5173:5173"
    environment:
      - CHOKIDAR_USEPOLLING=true
      - FAST_REFRESH=true
      - VITE_BACKEND_URL=http://localhost:3001
    volumes:
      - ./frontend:/app
      - frontend_node_modules:/app/node_modules
    stdin_open: true
    tty: true

volumes:
  frontend_node_modules:

