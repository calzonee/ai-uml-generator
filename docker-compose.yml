version: "3.9"
services:
  ollama:
    # a) offizielle Variante
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MODEL=llama3:8b
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      interval: 5s
      retries: 20
      start_period: 10s
    restart: unless-stopped

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    image: project-backend:latest
    container_name: chatuml-backend
    ports:
      - "3001:3000"
    environment:
      - LLAMA_MODEL=llama3:8b
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - ollama
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: chatuml-frontend
    ports:
      - "5173:5173"
    environment:
      - CHOKIDAR_USEPOLLING=true
      - FAST_REFRESH=true
      - VITE_BACKEND_URL=http://localhost:3001

    volumes:
      - ./frontend:/app
      - frontend_node_modules:/app/node_modules

    stdin_open: true
    tty: true

volumes:
  ollama_data:
  frontend_node_modules:
